{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ae3c81-56f4-44e3-b497-63be85c27334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[ai_parse_document関数](https://qiita.com/taka_yayoi/items/519a4b789d08290120fd)<br>\n",
    "[PDFデータソース](https://www.mckinsey.com/jp/~/media/mckinsey/locations/asia/japan/our%20insights/the_economic_potential_of_generative_ai_the_next_productivity_frontier_colormama_4k.pdf)<br>\n",
    "[非構造化 データパイプライン](https://docs.databricks.com/aws/ja/generative-ai/tutorials/ai-cookbook/quality-data-pipeline-rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "022190c9-d5aa-4f9c-8e74-448dd470691d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade langchain langchain-text-splitters\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d06e048-bac7-41c1-b055-707d26f780cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d0e542e-9ef2-4ac0-9e3d-febdef13e03b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1 . テーブル / chunks 作成\n",
    "テキストデータをembeddingsモデルに合わせてチャンキングしてテーブル保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de61b84-e5bb-456f-8d86-a00b036ee545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from uuid import uuid4\n",
    "import pandas as pd\n",
    "\n",
    "# RecursiveCharacterTextSplitterの設定\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,                 # 必要に応じてチャンクサイズを変更してください\n",
    "    chunk_overlap=20,               # チャンク間の重複（オーバーラップ）部分のサイズ\n",
    "    keep_separator=True,            # チャンクの前後に区切り文字を残すかどうか\n",
    "    length_function=len,            # チャンクの長さ判定に使う関数\n",
    "    is_separator_regex=False        # 区切り文字を正規表現として扱うかどうか\n",
    ")\n",
    "\n",
    "# gold_feedbacksテーブルからデータ取得\n",
    "df = spark.table(f\"{catalog}.{schema}.gold_feedbacks\").toPandas()\n",
    "\n",
    "# チャンク処理およびテーブル作成用リスト\n",
    "rows = []\n",
    "for idx, row in df.iterrows():\n",
    "    # commentをchunkに分割\n",
    "    splitted_texts = text_splitter.split_text(str(row[\"comment\"]))\n",
    "    for chunk in splitted_texts:\n",
    "        rows.append({\n",
    "            \"chunk_id\": str(uuid4()),  # 一意のIDをchunkごとに生成\n",
    "            \"feedback_id\": row[\"feedback_id\"],\n",
    "            \"user_id\": row[\"user_id\"],\n",
    "            \"product_id\": row[\"product_id\"],\n",
    "            \"rating\": row[\"rating\"],\n",
    "            \"date\": row[\"date\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            \"positive_score\": row[\"positive_score\"],\n",
    "            \"summary\": row[\"summary\"],\n",
    "            \"chunk\": chunk\n",
    "        })\n",
    "\n",
    "# 新しいDataFrame作成\n",
    "df_final = pd.DataFrame(rows)\n",
    "\n",
    "# Spark DataFrame変換・テーブル保存\n",
    "df_final_spark = spark.createDataFrame(df_final)\n",
    "df_final_spark.write.mode('overwrite').format('delta') \\\n",
    "            .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "            .saveAsTable(f'{catalog}.{schema}.gold_feedbacks_chunks')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7409889069732507,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "04_VectorSearch_チャンク処理",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
